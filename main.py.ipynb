{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbdccdb",
   "metadata": {},
   "source": [
    "# Bitcoin AI trader\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To start make sure all dependencies are installed with this command `pip install numpy tensorflow matplotlib sqlalchemy pandas psycopg2 urllib3 scipy`\n",
    "\n",
    "## Fetch data\n",
    "\n",
    "To fetch the data run this command \n",
    "* `bash ./download-spot-klines.sh`\n",
    "* `bash ./unzip-all.sh`\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bec2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GlobalAveragePooling1D, Concatenate, Dropout, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scipy.stats.mstats import winsorize\n",
    "from math import ceil\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e373811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_model(action_size, shape):\n",
    "    input_layer = Input(shape=shape)\n",
    "\n",
    "    # Convolutional Branch\n",
    "    conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
    "    conv_branch = BatchNormalization()(conv_branch)\n",
    "    conv_branch = Dropout(0.3)(conv_branch)\n",
    "    conv_branch = GlobalAveragePooling1D()(conv_branch)\n",
    "\n",
    "    # LSTM Branch\n",
    "    lstm_branch = LSTM(50, return_sequences=True)(input_layer)\n",
    "    lstm_branch = BatchNormalization()(lstm_branch)\n",
    "    lstm_branch = Dropout(0.3)(lstm_branch)\n",
    "    lstm_branch = GlobalAveragePooling1D()(lstm_branch)\n",
    "\n",
    "    # Combining both branches\n",
    "    concatenated = Concatenate()([conv_branch, lstm_branch])\n",
    "    dense = Dense(100, activation='relu', kernel_regularizer=l2(0.01))(concatenated)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    dense = Dense(25, activation='relu', kernel_regularizer=l2(0.01))(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    output = Dense(action_size, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, actions, rewards, batch_size):\n",
    "    total_samples = len(data)\n",
    "    while True:\n",
    "        for start in range(0, total_samples, batch_size):\n",
    "            end = min(start + batch_size, total_samples)\n",
    "            yield data[start:end], actions[start:end], rewards[start:end]\n",
    "\n",
    "def train_with_accumulation(model, data_gen, steps_per_epoch, accumulation_steps=4, epochs=10):\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        gradient_accumulation = [tf.zeros_like(v) for v in model.trainable_variables]\n",
    "        count = 0\n",
    "        for batch_data, batch_actions, batch_rewards in data_gen:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_data, training=True)\n",
    "                loss = loss_fn(batch_rewards, predictions)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            gradient_accumulation = [accum + grad for accum, grad in zip(gradient_accumulation, grads)]\n",
    "            count += 1\n",
    "\n",
    "            if count % accumulation_steps == 0 or count == steps_per_epoch:\n",
    "                optimizer.apply_gradients(zip(gradient_accumulation, model.trainable_variables))\n",
    "                gradient_accumulation = [tf.zeros_like(v) for v in model.trainable_variables]\n",
    "                count = 0\n",
    "\n",
    "            if count == steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / steps_per_epoch:.4f}\")\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files into a single DataFrame\n",
    "path = 'data/test'  # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    print(f\"Processing file: {filename}\")\n",
    "    data = pd.read_csv(filename, index_col=None, header=0)\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after importing from csv before conversion to numeric\")\n",
    "\n",
    "    data.columns = [\n",
    "        'Open time', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "        'Close time', 'Quote asset volume', 'Number of trades',\n",
    "        'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore'\n",
    "    ]\n",
    "\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after renaming columns\")\n",
    "\n",
    "    data['Open time'] = pd.to_datetime(data['Open time'], unit='ms')\n",
    "    data['Close time'] = pd.to_datetime(data['Close time'], unit='ms')\n",
    "\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after conversion to datetime\")\n",
    "\n",
    "    numeric_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Quote asset volume', 'Number of trades',\n",
    "                        'Taker buy base asset volume', 'Taker buy quote asset volume']\n",
    "\n",
    "    for feature in ['Open time', 'Close time']:\n",
    "        data[f'{feature} hour'] = data[feature].dt.hour\n",
    "        numeric_features.append(f'{feature} hour')\n",
    "        data[f'{feature} day of week'] = data[feature].dt.weekday\n",
    "        numeric_features.append(f'{feature} day of week')\n",
    "        data[f'{feature} week of year'] = data[feature].dt.isocalendar().week.astype(int)\n",
    "        numeric_features.append(f'{feature} week of year')\n",
    "        data[f'{feature} month'] = data[feature].dt.month\n",
    "        # numeric_features.append(f'{feature} month')\n",
    "\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after setting time features\")\n",
    "\n",
    "    data[numeric_features] = data[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "    print(\"After conversion to numeric: \", data[numeric_features].isna().sum())\n",
    "\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after conversion to numeric\")\n",
    "\n",
    "    print(\"Before SMA/LMA: \", data['Close'].isna().sum())\n",
    "    data['SMA'] = data['Close'].rolling(window=300).mean()\n",
    "    data['LMA'] = data['Close'].rolling(window=600).mean()\n",
    "    first_valid_close = data['Close'].dropna().iloc[0]\n",
    "    data['SMA'] = data['SMA'].fillna(first_valid_close)\n",
    "    data['LMA'] = data['LMA'].fillna(first_valid_close)\n",
    "    print(\"After SMA/LMA: \", data[['SMA', 'LMA']].isna().sum())\n",
    "\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after calculating SMA/LMA\")\n",
    "\n",
    "    threshold = 0.01\n",
    "    data['action'] = np.where(\n",
    "        (data['SMA'] > data['LMA']) & ((data['SMA'] - data['LMA']) / data['LMA'] > threshold), 1,\n",
    "        np.where((data['SMA'] < data['LMA']) & ((data['LMA'] - data['SMA']) / data['SMA'] > threshold), -1, 0)\n",
    "    )\n",
    "\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after setting action\")\n",
    "\n",
    "    transaction_cost = 0.0005\n",
    "    data['reward'] = data['Close'].diff().shift(-1) - (data['Close'] * transaction_cost)\n",
    "    data['reward'].fillna(0, inplace=True)\n",
    "    print(\"After reward: \", data['reward'].isna().sum())\n",
    "\n",
    "    data[numeric_features] = data[numeric_features].astype(float)\n",
    "\n",
    "    # Assuming data is now numeric and can be winsorized\n",
    "    print(\"Before winsorizing: \", data[numeric_features].isna().sum())\n",
    "    data[numeric_features] = data[numeric_features].apply(lambda x: winsorize(x, limits=[0.01, 0.01]))\n",
    "    print(\"After winsorizing: \", data[numeric_features].isna().sum())\n",
    "\n",
    "    # Min-max scaling\n",
    "    min_max_scaler = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "    data[numeric_features] = data[numeric_features].apply(min_max_scaler)\n",
    "    # remove timestamp features\n",
    "    data.drop(columns=['Open time', 'Close time'], inplace=True)\n",
    "    features = np.concatenate((numeric_features, ['Open time month', 'Close time month']))\n",
    "    print(\"features: \", features)\n",
    "    print(\"After scaling: \", data[features].isna().sum())\n",
    "\n",
    "    if np.any(np.isnan(data)):\n",
    "        print(\"NaNs found after preprocessing\")\n",
    "\n",
    "    li.append(data)\n",
    "\n",
    "combined_data = pd.concat(li, axis=0, ignore_index=True)\n",
    "actions = combined_data['action'].dropna().astype(int).values\n",
    "rewards = combined_data['reward'].dropna().astype(float).values\n",
    "\n",
    "# Assuming data is ready to be transformed and used in model\n",
    "data = np.expand_dims(combined_data.to_numpy(), axis=0)\n",
    "actions = np.expand_dims(actions, axis=0)\n",
    "rewards = np.expand_dims(rewards, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting Function\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['loss'], label='Training loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Train the model\n",
    "# model = train_model(model, data, actions, rewards, epochs=10, batch_size=16)\n",
    "# plot_history(history)\n",
    "# model.summary()\n",
    "\n",
    "# Setup for using the data and training the model\n",
    "# This is an example, modify according to how your actual data is structured and needs to be processed.\n",
    "# data = np.expand_dims(data, axis=0)\n",
    "# data = np.repeat(data, 16, axis=0)\n",
    "steps_per_epoch = ceil(len(data[0]) / 16)\n",
    "\n",
    "model = create_optimized_model(action_size=3, shape=(data.shape[1], data.shape[2]))\n",
    "# model.fit(data, actions, batch_size=16, epochs=10)\n",
    "data = data.astype('float32')\n",
    "data_gen = data_generator(data, actions, rewards, batch_size=16)\n",
    "train_with_accumulation(model, data_gen, steps_per_epoch, accumulation_steps=4, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
