{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy tensorflow matplotlib sqlalchemy pandas psycopg2 urllib3 scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bec2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Flatten, Concatenate, Dropout, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scipy.stats.mstats import winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load all CSV files into a single DataFrame\n",
    "path = 'data/klines'  # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df.columns = ['Open time','Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume','Ignore']\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# Step 2: Convert timestamps to readable dates and type casting\n",
    "frame['Open time'] = pd.to_datetime(frame['Open time'], unit='ms')\n",
    "frame['Close time'] = pd.to_datetime(frame['Close time'], unit='ms')\n",
    "\n",
    "# Adding additional time features\n",
    "time_features = ['Open time', 'Close time']\n",
    "for feature in time_features:\n",
    "    frame[f'{feature} hour'] = frame[feature].dt.hour\n",
    "    frame[f'{feature} day of week'] = frame[feature].dt.weekday\n",
    "    frame[f'{feature} week of year'] = frame[feature].dt.isocalendar().week\n",
    "    frame[f'{feature} month'] = frame[feature].dt.month\n",
    "\n",
    "# Continue with type casting for other columns\n",
    "for col in ['Open', 'High', 'Low', 'Close', 'Volume', 'Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume']:\n",
    "    frame[col] = frame[col].astype(float)\n",
    "\n",
    "# Normalize using Min-Max Scaling\n",
    "min_max_scaler = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "frame[['Open', 'High', 'Low', 'Close', 'Volume', 'Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume']] = frame[['Open', 'High', 'Low', 'Close', 'Volume', 'Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume']].apply(min_max_scaler)\n",
    "\n",
    "# Optionally, handle outliers\n",
    "# from scipy.stats.mstats import winsorize\n",
    "frame[['Open', 'High', 'Low', 'Close', 'Volume']] = frame[['Open', 'High', 'Low', 'Close', 'Volume']].apply(lambda x: winsorize(x, limits=[0.01, 0.01]))\n",
    "\n",
    "# Example of preparing data for LSTM or other models\n",
    "# This part would depend on how you want to use these features in your model setup\n",
    "features = frame[['Open', 'High', 'Low', 'Close', 'Volume', 'Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Open time hour', 'Open time day of week', 'Open time week of year', 'Open time month', 'Close time hour', 'Close time day of week', 'Close time week of year', 'Close time month']].values\n",
    "# target and data preparation for LSTM or model would go here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e373811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_model(window_size, feature_size, action_size):\n",
    "    input_layer = Input(shape=(window_size, feature_size))\n",
    "    \n",
    "    # Convolutional Branch\n",
    "    conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
    "    conv_branch = BatchNormalization()(conv_branch)\n",
    "    conv_branch = Dropout(0.3)(conv_branch)\n",
    "    conv_branch = Flatten()(conv_branch)\n",
    "    \n",
    "    # LSTM Branch\n",
    "    lstm_branch = LSTM(50, return_sequences=True)(input_layer)\n",
    "    lstm_branch = BatchNormalization()(lstm_branch)\n",
    "    lstm_branch = Dropout(0.3)(lstm_branch)\n",
    "    lstm_branch = Flatten()(lstm_branch)\n",
    "    \n",
    "    # Combining both branches\n",
    "    concatenated = Concatenate()([conv_branch, lstm_branch])\n",
    "    dense = Dense(100, activation='relu', kernel_regularizer=l2(0.01))(concatenated)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    dense = Dense(25, activation='relu', kernel_regularizer=l2(0.01))(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    output = Dense(action_size, activation='sigmoid')(dense)  # Changed to 'action_size' for generalization\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training Function Modified for Generalization and Efficiency\n",
    "def train_model(model, data, actions, rewards, epochs=10, batch_size=32):\n",
    "    action_size = model.output_shape[-1]  # Generalizing for any action size\n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for start_idx in range(0, data.shape[0] - batch_size + 1, batch_size):\n",
    "            batch_data = data[start_idx:start_idx + batch_size]\n",
    "            batch_actions = actions[start_idx:start_idx + batch_size]\n",
    "            batch_rewards = rewards[start_idx:start_idx + batch_size]\n",
    "            \n",
    "            current_qs = model.predict(batch_data)\n",
    "            updated_qs = current_qs.copy()\n",
    "            for i in range(batch_size):\n",
    "                updated_qs[i, batch_actions[i]] = batch_rewards[i]  # Generalized for multi-output\n",
    "            \n",
    "            loss = model.train_on_batch(batch_data, updated_qs)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        avg_loss = np.mean(losses)\n",
    "        history['loss'].append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# Plotting Function\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['loss'], label='Training loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "window_size = 100  # example value, adjust based on your temporal input size\n",
    "feature_size = features.shape[1]  # from your existing 'features' array\n",
    "action_size = 3  # for example, if actions are coded as 0, 1, 2\n",
    "\n",
    "model = create_optimized_model(window_size, feature_size, action_size)\n",
    "\n",
    "history = train_model(model, data, actions, rewards)\n",
    "plot_history(history)\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
